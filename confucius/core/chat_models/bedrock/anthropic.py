# Copyright (c) Meta Platforms, Inc. and affiliates.
# pyre-strict

import logging
from typing import Any

from langchain_core.callbacks import (
    AsyncCallbackManagerForLLMRun,
    CallbackManagerForLLMRun,
)

from langchain_core.language_models import BaseChatModel
from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, SystemMessage
from langchain_core.outputs import ChatGeneration, ChatResult
from pydantic import Field

from ....utils.asyncio import convert_to_async
from ....utils.decorators import retryable, RETRYABLE_CONNECTION_ERRS

from .api.invoke_model import anthropic as ant

from .base import BedrockBase
from .exceptions import (
    bedrock_exception_handling,
    BedrockInvalidResponseException,
    BedrockModelErrorException,
    BedrockServiceUnavailableException,
    BedrockThrottlingException,
    BedrockValidationException,
    UnexpectedEmptyResponseException,
)
from .model_id import get_model_id
from .utils import (
    append_stop_sequence,
    lc_message_to_ant_message,
    lc_message_to_ant_system,
)

RETRYABLE_EXCEPTIONS: tuple[type[Exception], ...] = RETRYABLE_CONNECTION_ERRS + (
    BedrockServiceUnavailableException,
    BedrockThrottlingException,
    BedrockModelErrorException,
)


logger: logging.Logger = logging.getLogger(__name__)


class ClaudeChat(BedrockBase, BaseChatModel):
    """LangChain Wrapper around Bedrock Claude chat language model."""

    thinking: ant.Thinking | None = Field(default=None)
    tool_choice: ant.ToolChoice | None = Field(default=None)
    tools: list[ant.ToolLike] | None = Field(default=None)

    include_stop_sequence: bool = Field(
        default=True,
        description="Whether to include the stop sequence in the response.",
    )

    class Config:
        """Configuration for this pydantic object."""

        arbitrary_types_allowed = True

    @property
    def _llm_type(self) -> str:
        """Return type of llm."""
        return "claude-chat"

    @property
    def _is_thinking_enabled(self) -> bool:
        """Return whether the model supports thinking."""
        return (
            self.thinking is not None and self.thinking.type == ant.ThinkingType.ENABLED
        )

    async def _get_invoke_model_request_body(
        self, input_messages: list[BaseMessage]
    ) -> ant.Body:
        """Get the request body for the invoke_model request."""
        body = ant.Body(
            anthropic_version=self.version,
            anthropic_beta=self.beta,
            stop_sequences=self.stop,
            temperature=1.0 if self._is_thinking_enabled else self.temperature,
            top_p=None if self._is_thinking_enabled else self.top_p,
            max_tokens=self.max_tokens or 8192,
            thinking=self.thinking,
            tool_choice=self.tool_choice,
            tools=self.tools,
        )
        for msg in input_messages:
            last_role: ant.MessageRole | None = (
                body.messages[-1].role if body.messages else None
            )
            if isinstance(msg, SystemMessage):
                body.system = lc_message_to_ant_system(msg)

            elif isinstance(msg, (HumanMessage, AIMessage)):
                ant_msg = await lc_message_to_ant_message(msg)
                if last_role == ant_msg.role:
                    body.messages[-1].content.extend(ant_msg.content)
                else:
                    body.messages.append(ant_msg)

        return body

    def _get_serialized_body(self, body: ant.Body) -> str:
        """Serialize the body for the invoke_model request."""
        return body.model_dump_json(exclude_none=True)

    def _parse_response_body(self, response_body: str) -> ant.Response:
        """Parse the response from the invoke_model request."""
        return ant.Response.parse_raw(response_body)

    async def _invoke_model(self, body: ant.Body) -> ant.Response:
        """Invoke the model and return the response."""
        with bedrock_exception_handling():
            modelId = get_model_id(self.model)
            logger.info(f"Sending bedrock request to {modelId}")
            raw_response = await convert_to_async(
                self.client.invoke_model,
                modelId=modelId,
                body=self._get_serialized_body(body),
            )

        try:
            response = self._parse_response_body(raw_response["body"].read())
        except Exception as e:
            raise BedrockInvalidResponseException(
                f"Failed to parse response: {e}",
                metadata=raw_response.get("ResponseMetadata"),
            ) from e

        response.metadata = raw_response.get("ResponseMetadata")
        if response.stop_reason == ant.StopReason.END_TURN and not response.content:
            # when END_TURN is the stop reason, the response can not be empty
            raise UnexpectedEmptyResponseException(
                "Empty response with END_TURN stop reason", metadata=response.metadata
            )
        return response

    def _generate(
        self,
        messages: list[BaseMessage],
        stop: list[str] | None = None,
        run_manager: CallbackManagerForLLMRun | None = None,
        **kwargs: Any,
    ) -> ChatResult:
        raise NotImplementedError(
            "Sync version _generate is not recommended for ClaudeChat"
        )

    async def _agenerate(
        self,
        messages: list[BaseMessage],
        stop: list[str] | None = None,
        run_manager: AsyncCallbackManagerForLLMRun | None = None,
        **kwargs: Any,
    ) -> ChatResult:
        body = await self._get_invoke_model_request_body(messages)

        @retryable(exceptions=RETRYABLE_EXCEPTIONS, **self.retryable_config.dict())
        async def _invoke_model_with_retry(body: ant.Body) -> ant.Response:
            return await self._invoke_model(body)

        response = await _invoke_model_with_retry(body)

        if self.include_stop_sequence:
            response = append_stop_sequence(response)

        return ChatResult(
            generations=[
                ChatGeneration(
                    message=AIMessage(
                        content=[ct.dict(exclude_none=True) for ct in response.content],
                        response_metadata=response.dict(exclude_none=True),
                    ),
                )
            ]
        )
